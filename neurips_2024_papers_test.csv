format,title,authors,abstract,url
Poster,Robust Conformal Prediction Using Privileged Information,"['Shai Feldman', 'Yaniv Romano']","We develop a method to generate prediction sets with a guaranteed coverage rate that is robust to corruptions in the training data, such as missing or noisy variables. Our approach builds on conformal prediction, a powerful framework to construct prediction sets that are valid under the i.i.d assumption. Importantly, naively applying conformal prediction does not provide reliable predictions in this setting, due to the distribution shift induced by the corruptions. To account for the distribution shift, we assume access to privileged information (PI). The PI is formulated as additional features that explain the distribution shift, however, they are only available during training and absent at test time.We approach this problem by introducing a novel generalization of weighted conformal prediction and support our method with theoretical coverage guarantees. Empirical experiments on both real and synthetic datasets indicate that our approach achieves a valid coverage rate and constructs more informative predictions compared to existing methods, which are not supported by theoretical guarantees.",https://nips.cc/virtual/2024/poster/93870
Poster,Entropy testing and its application to testing Bayesian networks,"['Clément L Canonne', 'Qiping Yang']","This paper studies the problem of \emph{entropy identity testing}: given sample access to a distribution p
and a fully described distribution q
, and the promise that either p=q
or |H(p)−H(q)|⩾ε
, where H(⋅)
denotes the Shannon entropy, a tester needs to distinguish between the two cases with high probability. We establish a near-optimal sample complexity bound of
˜
Θ
(
√
k
/ε+1/ε2
) for this problem, and show how to apply it to the problem of identity testing for in-degree-d
n
-dimensional Bayesian networks, obtaining an upper bound of
˜
O
(2d/2n/ε2+n2/ε4)
. This improves on the sample complexity bound of
˜
O
(2d/2n2/ε4)
from Canonne, Diakonikolas, Kane, and Stewart (2020), which required an additional assumption on the structure of the (unknown) Bayesian network.",https://nips.cc/virtual/2024/poster/94492
Poster,Mind the Graph When Balancing Data for Fairness or Robustness,"['Jessica Schrouff', 'Alexis Bellot', 'Amal Rannen-Triki', 'Alan Malek', 'Isabela Albuquerque', 'Arthur Gretton', ""Alexander D'Amour"", 'Silvia Chiappa']","Failures of fairness or robustness in machine learning predictive settings can be due to undesired dependencies between covariates, outcomes and auxiliary factors of variation. A common strategy to mitigate these failures is data balancing, which attempts to remove those undesired dependencies. In this work, we define conditions on the training distribution for data balancing to lead to fair or robust models. Our results display that in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization. Overall, our results highlight the importance of taking the causal graph into account before performing data balancing.",https://nips.cc/virtual/2024/poster/95592
